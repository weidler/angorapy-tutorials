{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with AngoraPy\n",
    "\n",
    "In this tutorial we will introduce you to the basic functionality of AngoraPy. We will cover the full workflow from creating an environment, to building the model, to combining them in an agent, through to training and evaluating that agent. We will take all of these steps with no major customization so that we can focus on the overall structure of applying AngoraPy. Customizing specifically the task and the model is covered in other notebooks in this same repository.\n",
    "\n",
    "## Installation\n",
    "Before you build your first agent in AngoraPy, you need to install the package. Since AngoraPy depends on a multitude of other packages and their specific versions, we recommend doing a clean installation in a new virtual environment. In this environment, first install some build dependencies as follows:\n",
    "\n",
    "    pip install swig imageio\n",
    "\n",
    "and then install AngoraPy itself.\n",
    "\n",
    "    pip install angorapy\n",
    "\n",
    "You now have all you need to build an agent.\n",
    "\n",
    "## Your First Agent in AngoraPy\n",
    "\n",
    "We begin by importing angorapy, and numpy for basic operations. Additionally, we turn off tensorflow's logging to keep outputs clean."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:59:52.987239Z",
     "start_time": "2024-08-19T12:59:52.982714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import angorapy as ap"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For most environments, PPO needs to normalize states and rewards; to add this functionality we wrap the environment with transformers fulfilling this task. You can also add your own custom transformers this way."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:59:53.009914Z",
     "start_time": "2024-08-19T12:59:53.003701Z"
    }
   },
   "source": "env = ap.make_env(\"CartPole-v1\")",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the policy distribution we would like to model to map to. We will use a coategorical distribution. Since the distribution will depend on the action space of the environment, we need to provide the distribution with the environment object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:59:53.065461Z",
     "start_time": "2024-08-19T12:59:53.061054Z"
    }
   },
   "source": "distribution = ap.policies.CategoricalPolicyDistribution(env)",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a model. To that end, we create a *model builder*. AngoraPy needs to be able to constantly build new versions of the model. Thus, it requires a model building function instead of a model instance. This function must return a tuple of (policy, value, joint) network. The former are the network selecting the action (policy network) and valuating the state (value network. The latter is their combination. The separation of the three serves computational efficiency.\n",
    "\n",
    "For built in architectures, we can use the *get_model_builder()* function. Lets also check the models this model builder creates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:59:54.881134Z",
     "start_time": "2024-08-19T12:59:53.115167Z"
    }
   },
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "build_models = ap.models.get_model_builder(model=\"simple\", model_type=\"ffn\", shared=False)\n",
    "policy, value, joint = build_models(env, distribution)\n",
    "\n",
    "plot_model(joint)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model created three network references. Importantly, it is references, as any change to the value or policy network will also change the joint network and vice versa. In the model plot, we can also see how policy and value network are separated. They only share their input, but not their weights.\n",
    "\n",
    "With model, environment and distribution set up, we can now assemble an agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T12:59:55.753149Z",
     "start_time": "2024-08-19T12:59:54.930402Z"
    }
   },
   "source": "agent = ap.Agent(build_models, env, horizon=1024, workers=1, distribution=distribution)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the agent for 10 cycle and afterwards save the final state. AngoraPy will additionally always save the agents best version."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:01:27.774199Z",
     "start_time": "2024-08-19T12:59:55.765011Z"
    }
   },
   "source": [
    "agent.drill(n=10, epochs=3, batch_size=64)\n",
    "agent.save_agent_state()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Drill started using 1 processes for 1 workers of which 1 are optimizers. Worker distribution: [1].\n",
      "IDs over Workers: [[0]]\n",
      "IDs over Optimizers: [[0]]\n",
      "Gathering cycle 0..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mBefore Training\u001B[0m; r: \u001B[91m   24.14\u001B[0m; len: \u001B[94m   24.14\u001B[0m; n: \u001B[94m 42\u001B[0m; loss: [\u001B[94m  pi  \u001B[0m|\u001B[94m  v     \u001B[0m|\u001B[94m  ent \u001B[0m]; upd: \u001B[94m     0\u001B[0m; y.exp: \u001B[94m0.000\u001B[0m; ; time:  ; time left: \u001B[94munknown time\u001B[0m; took s [unknown time left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 1..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     1/10\u001B[0m; r: \u001B[91m   25.25\u001B[0m; len: \u001B[94m   25.25\u001B[0m; n: \u001B[94m 40\u001B[0m; loss: [\u001B[94m -0.01\u001B[0m|\u001B[94m    0.58\u001B[0m|\u001B[94m  0.69\u001B[0m]; upd: \u001B[94m    48\u001B[0m; ; time: [8.6|0.0|1.6] [84|0|16]; time left: \u001B[94m1.5mins\u001B[0m; took 9.75s [1.5mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 2..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     2/10\u001B[0m; r: \u001B[91m   36.11\u001B[0m; len: \u001B[94m   36.11\u001B[0m; n: \u001B[94m 28\u001B[0m; loss: [\u001B[94m -0.09\u001B[0m|\u001B[94m    0.25\u001B[0m|\u001B[94m  0.66\u001B[0m]; upd: \u001B[94m    96\u001B[0m; ; time: [7.7|0.0|1.0] [88|0|12]; time left: \u001B[94m1.2mins\u001B[0m; took 8.66s [1.2mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 3..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     3/10\u001B[0m; r: \u001B[91m   60.06\u001B[0m; len: \u001B[94m   60.06\u001B[0m; n: \u001B[94m 17\u001B[0m; loss: [\u001B[94m -0.02\u001B[0m|\u001B[94m    0.22\u001B[0m|\u001B[94m  0.65\u001B[0m]; upd: \u001B[94m   144\u001B[0m; ; time: [7.3|0.0|1.0] [88|0|12]; time left: \u001B[94m1.1mins\u001B[0m; took 9.62s [1.1mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 4..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     4/10\u001B[0m; r: \u001B[91m   98.89\u001B[0m; len: \u001B[94m   98.89\u001B[0m; n: \u001B[94m  9\u001B[0m; loss: [\u001B[94m -0.01\u001B[0m|\u001B[94m    0.17\u001B[0m|\u001B[94m  0.61\u001B[0m]; upd: \u001B[94m   192\u001B[0m; ; time: [8.2|0.0|1.0] [89|0|11]; time left: \u001B[94m1.0mins\u001B[0m; took 10.11s [1.0mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 5..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     5/10\u001B[0m; r: \u001B[91m  216.25\u001B[0m; len: \u001B[94m  216.25\u001B[0m; n: \u001B[94m  4\u001B[0m; loss: [\u001B[94m -0.03\u001B[0m|\u001B[94m    0.12\u001B[0m|\u001B[94m  0.60\u001B[0m]; upd: \u001B[94m   240\u001B[0m; ; time: [8.8|0.0|1.0] [90|0|10]; time left: \u001B[94m0.8mins\u001B[0m; took 9.24s [0.8mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 6..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     6/10\u001B[0m; r: \u001B[91m  186.20\u001B[0m; len: \u001B[94m  186.20\u001B[0m; n: \u001B[94m  5\u001B[0m; loss: [\u001B[94m -0.06\u001B[0m|\u001B[94m    0.07\u001B[0m|\u001B[94m  0.58\u001B[0m]; upd: \u001B[94m   288\u001B[0m; ; time: [7.9|0.0|1.0] [89|0|11]; time left: \u001B[94m0.6mins\u001B[0m; took 8.53s [0.6mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 7..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     7/10\u001B[0m; r: \u001B[91m  137.29\u001B[0m; len: \u001B[94m  137.29\u001B[0m; n: \u001B[94m  7\u001B[0m; loss: [\u001B[94m  0.02\u001B[0m|\u001B[94m    0.05\u001B[0m|\u001B[94m  0.55\u001B[0m]; upd: \u001B[94m   336\u001B[0m; ; time: [7.3|0.0|1.0] [88|0|12]; time left: \u001B[94m0.5mins\u001B[0m; took 8.78s [0.5mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 8..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     8/10\u001B[0m; r: \u001B[91m  160.00\u001B[0m; len: \u001B[94m  160.00\u001B[0m; n: \u001B[94m  6\u001B[0m; loss: [\u001B[94m -0.02\u001B[0m|\u001B[94m    0.02\u001B[0m|\u001B[94m  0.57\u001B[0m]; upd: \u001B[94m   384\u001B[0m; ; time: [7.5|0.0|0.9] [89|0|11]; time left: \u001B[94m0.3mins\u001B[0m; took 8.44s [0.3mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 9..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     9/10\u001B[0m; r: \u001B[91m  198.50\u001B[0m; len: \u001B[94m  198.50\u001B[0m; n: \u001B[94m  4\u001B[0m; loss: [\u001B[94m -0.08\u001B[0m|\u001B[94m    0.02\u001B[0m|\u001B[94m  0.54\u001B[0m]; upd: \u001B[94m   432\u001B[0m; ; time: [7.3|0.0|1.0] [88|0|12]; time left: \u001B[94m0.2mins\u001B[0m; took 8.28s [0.2mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing...Drill finished after 91.72serialization.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we can evaluate the agent. For this purpose we tell the agent to *act confidently*. Because policies in AngoraPy are stochastic, actions are usually sampled from the policy distribution. At evaluation time, we would however prefer the agent to stop exploring and instead choose the action it is most confident about. Thus, when told to act confidently, the agent will not sample but instead choose the most likely action under the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:01:33.178376Z",
     "start_time": "2024-08-19T13:01:27.796509Z"
    }
   },
   "source": [
    "evaluation_results = agent.evaluate(10, act_confidently=True)[0]\n",
    "print(np.mean(evaluation_results.episode_rewards))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see (usually) performance is higher than after the last optimization, because the agent is not exploring anymore."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:01:33.202899Z",
     "start_time": "2024-08-19T13:01:33.199459Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
