{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building Custom Models\n",
    "\n",
    "When you are using AngoraPy to train goal-driven models of the brain, you will usually want these models to be _your_ custom networks. In this notebook, we show you the basics of constructing and registering your own model.\n",
    "\n",
    "We first import necessary dependencies and then, like in the previous tutorial, build the environment and distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:07:22.240314Z",
     "start_time": "2024-08-19T13:07:22.233415Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import angorapy as ap\n",
    "\n",
    "env = ap.make_env(\"CartPole-v1\")\n",
    "distribution = ap.policies.CategoricalPolicyDistribution(env)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to, you can also go for a different environment, for instance `LunarLander-v2`. However, training it will need a little more time and potentially a stronger network.\n",
    "\n",
    "We now will build the model. In AngoraPy, we do not handle models itself, but instead operate on _model builders_. The reason for this is practicality in the backend of the library. Because we use truncated backpropagation through time, models need to be stateful (for an explanation of this, check the paper introducing AngoraPy). This requires models to be build with a specific sequence length. However, when collecting data, we want to do single steps, whereas we optimize on longer sequences. This requires us to constantly rebuild the model, thus demanding functions instead of objects. \n",
    "\n",
    "Anyways. The functions we write to build our models have some requirements.\n",
    "\n",
    "1. Their signature mus follow the format `function(env, distribution, bs, sequence_length) -> policy, value, joint` where `env` and `distribution` are the environment and distribution the model will act upon, and `policy`, `value` and `joint` are the models (more about this below).\n",
    "2. All recurrent elements need to return sequences and be stateful. In this notebook, however, we will start with a simple feedforward network. Only in the following notebook will we integrate a recurrent part.\n",
    "3. It has to be registered in angorapy for later reference using th `ap.models.register_model(\"MODELNAME\")` decorator.\n",
    "\n",
    "We write a function builder for a 5-layer network, which partially shares weights between policy and value network. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:07:22.277974Z",
     "start_time": "2024-08-19T13:07:22.263199Z"
    }
   },
   "source": [
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from angorapy.utilities.model_utils import env_extract_dims, make_input_layers\n",
    "\n",
    "@ap.models.register_model(\"MyModel\")\n",
    "def build_my_amazing_model(env, distribution, bs=1, sequence_length=None):\n",
    "    inputs = make_input_layers(env, bs)[\"proprioception\"]\n",
    "    _, n_actions = env_extract_dims(env)\n",
    "\n",
    "    \n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    x_policy = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    x_policy = tf.keras.layers.Dense(8, activation=\"relu\")(x_policy)\n",
    "    x_value = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    x_value = tf.keras.layers.Dense(8, activation=\"relu\")(x_value)\n",
    "\n",
    "    out_policy = distribution.build_action_head(n_actions, x_policy.shape[1:], bs)(x_policy)\n",
    "    out_value = tf.keras.layers.Dense(1)(x_value)\n",
    "\n",
    "    policy = tf.keras.Model(inputs=inputs, outputs=out_policy, name=\"my_policy_function\")\n",
    "    value = tf.keras.Model(inputs=inputs, outputs=out_value, name=\"my_value_function\")\n",
    "    joint = tf.keras.Model(inputs=inputs, outputs=[out_policy, out_value], name=\"my_joint_networks\")\n",
    "\n",
    "    return policy, value, joint"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We then build the agent with our model function and plot the model for inspection. (Note that for the plotting part we need graphviz and pyplot installed on our machine)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T13:07:23.254835Z",
     "start_time": "2024-08-19T13:07:22.292316Z"
    }
   },
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "agent = ap.Agent(build_my_amazing_model, env, horizon=2048, workers=1, distribution=distribution)\n",
    "plot_model(agent.joint)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats all we need to now train our model on the task, as we have previously seen in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:11:36.368611Z",
     "start_time": "2024-08-19T13:07:23.313688Z"
    }
   },
   "source": [
    "agent.drill(n=5, epochs=10, batch_size=32)\n",
    "agent.save_agent_state()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Drill started using 1 processes for 1 workers of which 1 are optimizers. Worker distribution: [1].\n",
      "IDs over Workers: [[0]]\n",
      "IDs over Optimizers: [[0]]\n",
      "Gathering cycle 0..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mBefore Training\u001B[0m; r: \u001B[91m   21.03\u001B[0m; len: \u001B[94m   21.03\u001B[0m; n: \u001B[94m 97\u001B[0m; loss: [\u001B[94m  pi  \u001B[0m|\u001B[94m  v     \u001B[0m|\u001B[94m  ent \u001B[0m]; upd: \u001B[94m     0\u001B[0m; y.exp: \u001B[94m0.000\u001B[0m; ; time:  ; time left: \u001B[94munknown time\u001B[0m; took s [unknown time left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 1..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     1/10\u001B[0m; r: \u001B[91m   40.14\u001B[0m; len: \u001B[94m   40.14\u001B[0m; n: \u001B[94m 51\u001B[0m; loss: [\u001B[94m  0.03\u001B[0m|\u001B[94m    0.43\u001B[0m|\u001B[94m  0.68\u001B[0m]; upd: \u001B[94m   640\u001B[0m; ; time: [16.3|0.0|9.4] [63|0|37]; time left: \u001B[94m3.7mins\u001B[0m; took 24.76s [3.7mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 2..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     2/10\u001B[0m; r: \u001B[91m   56.50\u001B[0m; len: \u001B[94m   56.50\u001B[0m; n: \u001B[94m 36\u001B[0m; loss: [\u001B[94m  0.07\u001B[0m|\u001B[94m    0.31\u001B[0m|\u001B[94m  0.64\u001B[0m]; upd: \u001B[94m  1280\u001B[0m; ; time: [15.0|0.0|8.4] [64|0|36]; time left: \u001B[94m3.3mins\u001B[0m; took 25.23s [3.3mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 3..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     3/10\u001B[0m; r: \u001B[91m   92.62\u001B[0m; len: \u001B[94m   92.62\u001B[0m; n: \u001B[94m 21\u001B[0m; loss: [\u001B[94m  0.02\u001B[0m|\u001B[94m    0.17\u001B[0m|\u001B[94m  0.60\u001B[0m]; upd: \u001B[94m  1920\u001B[0m; ; time: [16.5|0.0|8.8] [65|0|35]; time left: \u001B[94m2.9mins\u001B[0m; took 25.1s [2.9mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 4..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     4/10\u001B[0m; r: \u001B[91m  136.36\u001B[0m; len: \u001B[94m  136.36\u001B[0m; n: \u001B[94m 14\u001B[0m; loss: [\u001B[94m -0.04\u001B[0m|\u001B[94m    0.10\u001B[0m|\u001B[94m  0.57\u001B[0m]; upd: \u001B[94m  2560\u001B[0m; ; time: [16.0|0.0|9.1] [64|0|36]; time left: \u001B[94m2.5mins\u001B[0m; took 25.29s [2.5mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 5..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     5/10\u001B[0m; r: \u001B[33m  328.00\u001B[0m; len: \u001B[94m  328.00\u001B[0m; n: \u001B[94m  6\u001B[0m; loss: [\u001B[94m -0.10\u001B[0m|\u001B[94m    0.06\u001B[0m|\u001B[94m  0.53\u001B[0m]; upd: \u001B[94m  3200\u001B[0m; ; time: [15.9|0.0|9.5] [63|0|37]; time left: \u001B[94m2.1mins\u001B[0m; took 24.87s [2.1mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 6..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     6/10\u001B[0m; r: \u001B[91m  201.40\u001B[0m; len: \u001B[94m  201.40\u001B[0m; n: \u001B[94m 10\u001B[0m; loss: [\u001B[94m -0.05\u001B[0m|\u001B[94m    0.03\u001B[0m|\u001B[94m  0.49\u001B[0m]; upd: \u001B[94m  3840\u001B[0m; ; time: [15.1|0.0|8.3] [64|0|36]; time left: \u001B[94m1.7mins\u001B[0m; took 25.62s [1.7mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 7..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     7/10\u001B[0m; r: \u001B[91m  192.20\u001B[0m; len: \u001B[94m  192.20\u001B[0m; n: \u001B[94m 10\u001B[0m; loss: [\u001B[94m  0.01\u001B[0m|\u001B[94m    0.03\u001B[0m|\u001B[94m  0.55\u001B[0m]; upd: \u001B[94m  4480\u001B[0m; ; time: [17.1|0.0|9.2] [65|0|35]; time left: \u001B[94m1.3mins\u001B[0m; took 25.02s [1.3mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 8..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     8/10\u001B[0m; r: \u001B[91m  187.30\u001B[0m; len: \u001B[94m  187.30\u001B[0m; n: \u001B[94m 10\u001B[0m; loss: [\u001B[94m -0.01\u001B[0m|\u001B[94m    0.02\u001B[0m|\u001B[94m  0.53\u001B[0m]; upd: \u001B[94m  5120\u001B[0m; ; time: [15.5|0.0|8.9] [64|0|36]; time left: \u001B[94m0.8mins\u001B[0m; took 24.27s [0.8mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 9..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     9/10\u001B[0m; r: \u001B[91m  176.64\u001B[0m; len: \u001B[94m  176.64\u001B[0m; n: \u001B[94m 11\u001B[0m; loss: [\u001B[94m -0.08\u001B[0m|\u001B[94m    0.01\u001B[0m|\u001B[94m  0.54\u001B[0m]; upd: \u001B[94m  5760\u001B[0m; ; time: [15.1|0.0|9.1] [62|0|38]; time left: \u001B[94m0.4mins\u001B[0m; took 25.01s [0.4mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing...Drill finished after 252.73serialization.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, lets evaluate the agent to check how it performs without exploration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:11:37.285809Z",
     "start_time": "2024-08-19T13:11:36.446570Z"
    }
   },
   "source": [
    "evaluation_results = agent.evaluate(1, act_confidently=True)[0]\n",
    "print(f\"Mean performance after training: {np.mean(evaluation_results.episode_rewards)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance after training: 195.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we might want to skip the training at a later stage and instead just load a previously saved agent, lets see how this works. We can load an agent from one of the states it has been saved at by calling the static `Agent.from_agent_state()` method which acts as a constructor. When training an agent using the `drill()` method, your model will be saved at every cycle, once as the _last_ agent state and once as the _best_ agent state if it is performaing better than the previous best state. Both of them are constantly overwritten, such that there is always two saved states for an agent. However, you can manually save an agent calling `agent.save_agent_state()` and additionally instruct the drill method to save at some frequency. In the following, we load the agent at the default state, `\"best\"`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:11:37.522423Z",
     "start_time": "2024-08-19T13:11:37.420685Z"
    }
   },
   "source": "loaded_agent = ap.Agent.from_agent_state(agent.agent_id)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from iteration 9.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'build_my_amazing_model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m loaded_agent \u001B[38;5;241m=\u001B[39m ap\u001B[38;5;241m.\u001B[39mAgent\u001B[38;5;241m.\u001B[39mfrom_agent_state(agent\u001B[38;5;241m.\u001B[39magent_id)\n",
      "File \u001B[0;32m~/miniconda3/envs/angorapy-tutorials/lib/python3.11/site-packages/angorapy/agent/ppo_agent.py:1041\u001B[0m, in \u001B[0;36mPPOAgent.from_agent_state\u001B[0;34m(agent_id, from_iteration, force_env_name, path_modifier, n_optimizers)\u001B[0m\n\u001B[1;32m   1036\u001B[0m postprocessors \u001B[38;5;241m=\u001B[39m postprocessors_from_serializations(parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1037\u001B[0m env \u001B[38;5;241m=\u001B[39m make_task(parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv_name\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m force_env_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m force_env_name,\n\u001B[1;32m   1038\u001B[0m                 reward_config\u001B[38;5;241m=\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_configuration\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1039\u001B[0m                 postprocessors\u001B[38;5;241m=\u001B[39mpostprocessors,\n\u001B[1;32m   1040\u001B[0m                 render_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.*[Vv]is(ion|ual).*\u001B[39m\u001B[38;5;124m\"\u001B[39m, parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1041\u001B[0m model_builder \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mMODEL_BUILDERS[parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilder_function_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m   1042\u001B[0m distribution \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(policies, parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistribution\u001B[39m\u001B[38;5;124m\"\u001B[39m])(env)\n\u001B[1;32m   1044\u001B[0m loaded_agent \u001B[38;5;241m=\u001B[39m PPOAgent(model_builder, environment\u001B[38;5;241m=\u001B[39menv, horizon\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhorizon\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1045\u001B[0m                         workers\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_workers\u001B[39m\u001B[38;5;124m\"\u001B[39m], learning_rate\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   1046\u001B[0m                         discount\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiscount\u001B[39m\u001B[38;5;124m\"\u001B[39m], lam\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlam\u001B[39m\u001B[38;5;124m\"\u001B[39m], clip\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclip\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1050\u001B[0m                         lr_schedule\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr_schedule_type\u001B[39m\u001B[38;5;124m\"\u001B[39m], distribution\u001B[38;5;241m=\u001B[39mdistribution, _make_dirs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1051\u001B[0m                         reward_configuration\u001B[38;5;241m=\u001B[39mparameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_configuration\u001B[39m\u001B[38;5;124m\"\u001B[39m], n_optimizers\u001B[38;5;241m=\u001B[39mn_optimizers)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'build_my_amazing_model'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have fully recovered the agent, however this agent holds the weights from the best version of itself (the original those from the last state). Lets evaluate this agent's alter ego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(1, 4) dtype=float32 (created by layer 'proprioception')>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance after training: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = loaded_agent.evaluate(1, act_confidently=True)[0]\n",
    "print(f\"Mean performance after training: {np.mean(evaluation_results.episode_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the tutorial on model building. In the following notebook we will revisit the process, but show how to do it with a recurrent network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
