{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Custom Recurrent Models\n",
    "\n",
    "In the previous notebook we have looked at building custom models. We revisit this here, but this time the model is going to also be recurrent. It will be a very short tutorial, as the intricacy lies solely in the model building function. Everything else, i.e. handling the model, will be happening under the hood of AngoraPy, which automatically detects whether your model is recurrent and not, and then deals with it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:18:31.911433Z",
     "start_time": "2024-08-19T13:18:28.967015Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import angorapy as ap\n",
    "\n",
    "env = ap.make_env(\"CartPole-v1\")\n",
    "distribution = ap.policies.CategoricalPolicyDistribution(env)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we build is going to be quite similar to the previous one, however we replace the last shared feedforward layers by a shared GRU recurrent layer. AngoraPy's training algorithm requires you to make the recurrent layers both stateful and returning sequences. Additionally, you need to set the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:18:31.933126Z",
     "start_time": "2024-08-19T13:18:31.919399Z"
    }
   },
   "source": [
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from angorapy.utilities.model_utils import env_extract_dims\n",
    "\n",
    "@ap.models.register_model(\"MyModel\")\n",
    "def build_my_amazing_model(env, distribution, bs=1, sequence_length=1):\n",
    "    state_dimensionality, n_actions = env_extract_dims(env)\n",
    "\n",
    "    inputs = tf.keras.Input(batch_shape=(bs, sequence_length,) + state_dimensionality[\"proprioception\"], name=\"proprioception\")\n",
    "    masked = tf.keras.layers.Masking(batch_input_shape=(bs, sequence_length,) + (inputs.shape[-1], ))(inputs)\n",
    "\n",
    "    x = TimeDistributed(tf.keras.layers.Dense(8))(masked)\n",
    "\n",
    "    x, *_ = tf.keras.layers.GRU(4,\n",
    "                       stateful=True,\n",
    "                       return_sequences=True,\n",
    "                       return_state=True,\n",
    "                       batch_size=bs,\n",
    "                       name=\"policy_recurrent_layer\")(x)\n",
    "\n",
    "    x_policy = tf.keras.layers.Dense(8)(x)\n",
    "    x_value = tf.keras.layers.Dense(8)(x)\n",
    "\n",
    "    out_policy = distribution.build_action_head(n_actions, x_policy.shape[1:], bs)(x_policy)\n",
    "    out_value = tf.keras.layers.Dense(1)(x_value)\n",
    "\n",
    "    policy = tf.keras.Model(inputs=inputs, outputs=out_policy, name=\"my_policy_function\")\n",
    "    value = tf.keras.Model(inputs=inputs, outputs=out_value, name=\"my_value_function\")\n",
    "    joint = tf.keras.Model(inputs=inputs, outputs=[out_policy, out_value], name=\"my_joint_networks\")\n",
    "\n",
    "    return policy, value, joint"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wont load the model this time, so we can skip registering it. However, lets again plot the model after building the agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T13:18:35.641225Z",
     "start_time": "2024-08-19T13:18:31.998099Z"
    }
   },
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "agent = ap.Agent(build_my_amazing_model, env, horizon=2048, workers=1, distribution=distribution)\n",
    "plot_model(agent.joint)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now only training is left."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:21:22.230952Z",
     "start_time": "2024-08-19T13:18:35.753313Z"
    }
   },
   "source": [
    "agent.drill(n=5, epochs=3, batch_size=128)\n",
    "agent.save_agent_state()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Drill started using 1 processes for 1 workers of which 1 are optimizers. Worker distribution: [1].\n",
      "IDs over Workers: [[0]]\n",
      "IDs over Optimizers: [[0]]\n",
      "\n",
      "The policy is recurrent and the batch size is interpreted as the number of transitions per policy update. Given the batch size of 128 this results in: \n",
      "\t8 chunks per update and 16 updates per epoch\n",
      "\tBatch tilings of (1, 8) per process and (1, 8) in total.\n",
      "\n",
      "\n",
      "Gathering cycle 0..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mBefore Training\u001B[0m; r: \u001B[91m   20.89\u001B[0m; len: \u001B[94m   20.89\u001B[0m; n: \u001B[94m 74\u001B[0m; loss: [\u001B[94m  pi  \u001B[0m|\u001B[94m  v     \u001B[0m|\u001B[94m  ent \u001B[0m]; upd: \u001B[94m     0\u001B[0m; y.exp: \u001B[94m0.000\u001B[0m; ; time:  ; time left: \u001B[94munknown time\u001B[0m; took s [unknown time left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 1..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     1/5\u001B[0m; r: \u001B[91m   26.93\u001B[0m; len: \u001B[94m   26.93\u001B[0m; n: \u001B[94m 59\u001B[0m; loss: [\u001B[94m  9.90\u001B[0m|\u001B[94m    7.33\u001B[0m|\u001B[94m  5.54\u001B[0m]; upd: \u001B[94m    48\u001B[0m; ; time: [30.2|0.0|5.4] [85|0|15]; time left: \u001B[94m2.3mins\u001B[0m; took 34.42s [2.3mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 2..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     2/5\u001B[0m; r: \u001B[91m   31.33\u001B[0m; len: \u001B[94m   31.33\u001B[0m; n: \u001B[94m 49\u001B[0m; loss: [\u001B[94m 11.89\u001B[0m|\u001B[94m    6.71\u001B[0m|\u001B[94m  5.47\u001B[0m]; upd: \u001B[94m    96\u001B[0m; ; time: [28.6|0.0|3.2] [90|0|10]; time left: \u001B[94m1.6mins\u001B[0m; took 29.7s [1.6mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 3..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     3/5\u001B[0m; r: \u001B[91m   44.77\u001B[0m; len: \u001B[94m   44.77\u001B[0m; n: \u001B[94m 39\u001B[0m; loss: [\u001B[94m 17.41\u001B[0m|\u001B[94m    9.25\u001B[0m|\u001B[94m  5.25\u001B[0m]; upd: \u001B[94m   144\u001B[0m; ; time: [26.2|0.0|3.2] [89|0|11]; time left: \u001B[94m1.1mins\u001B[0m; took 32.87s [1.1mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering cycle 4..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[92mCycle     4/5\u001B[0m; r: \u001B[91m   95.47\u001B[0m; len: \u001B[94m   95.47\u001B[0m; n: \u001B[94m 19\u001B[0m; loss: [\u001B[94m  9.30\u001B[0m|\u001B[94m    5.26\u001B[0m|\u001B[94m  5.10\u001B[0m]; upd: \u001B[94m   192\u001B[0m; ; time: [29.4|0.0|3.3] [90|0|10]; time left: \u001B[94m0.5mins\u001B[0m; took 33.99s [0.5mins left]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing...Drill finished after 165.32serialization.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the drill function informed you about some details on the training that it did not include previously, because the model is recurrent. Because AngoraPy operates on temporal data, and specifically temporal chunks (as opposed to full sequences), it needs to convert the batch size you provide it with (which is the number of transitions included in every batch) into the number of chunks it processes per update. If we would distribute the training, it would additionally have to allocate chunks to the processes. \n",
    "\n",
    "Lets evaluate again. Most likely, training ended at a lower performance than our feedforward model. Thats because training recurrent policies is generally requiring more data, and for the given task we also do not need a memory. The state dynamics are already explicitly included as variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:21:26.038385Z",
     "start_time": "2024-08-19T13:21:22.246306Z"
    }
   },
   "source": [
    "evaluation_results = agent.evaluate(1, act_confidently=True)[0]\n",
    "print(f\"Mean performance after training: {np.mean(evaluation_results.episode_rewards)}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance after training: 285.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it for model building. Next, we will learn how to load and inspect agents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T13:21:26.055945Z",
     "start_time": "2024-08-19T13:21:26.052779Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
