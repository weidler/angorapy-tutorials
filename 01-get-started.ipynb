{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with AngoraPy\n",
    "\n",
    "In this tutorial we will introduce you to the basic functionality of AngoraPy. We will cover the full workflow from creating an environment, to building the model, to combining them in an agent, through to training and evaluating that agent. We will take all of these steps with no major customization so that we can focus on the overall structure of applying AngoraPy. Customizing specifically the task and the model is covered in other notebooks in this same repository.\n",
    "\n",
    "## Installation\n",
    "Before you build your first agent in AngoraPy, you need to install the package. Since AngoraPy depends on a multitude of other packages and their specific versions, we recommend doing a clean installation in a new virtual environment. In this environment, first install some build dependencies as follows:\n",
    "\n",
    "    pip install swig imageio\n",
    "\n",
    "and then install AngoraPy itself.\n",
    "\n",
    "    pip install angorapy\n",
    "\n",
    "You now have all you need to build an agent.\n",
    "\n",
    "## Your First Agent in AngoraPy\n",
    "\n",
    "We begin by importing angorapy, and numpy for basic operations. Additionally, we turn off tensorflow's logging to keep outputs clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import angorapy as apy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most environments, PPO needs to normalize states and rewards; to add this functionality we wrap the environment with transformers fulfilling this task. You can also add your own custom transformers this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = apy.make_env(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the policy distribution we would like to model to map to. We will use a coategorical distribution. Since the distribution will depend on the action space of the environment, we need to provide the distribution with the environment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = apy.policies.CategoricalPolicyDistribution(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need a model. To that end, we create a *model builder*. AngoraPy needs to be able to constantly build new versions of the model. Thus, it requires a model building function instead of a model instance. This function must return a tuple of (policy, value, joint) network. The former are the network selecting the action (policy network) and valuating the state (value network. The latter is their combination. The separation of the three serves computational efficiency.\n",
    "\n",
    "For built in architectures, we can use the *get_model_builder()* function. Lets also check the models this model builder creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "build_models = apy.get_model_builder(model=\"simple\", model_type=\"ffn\", shared=False)\n",
    "policy, value, joint = build_models(env, distribution)\n",
    "\n",
    "plot_model(joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model created three network references. Importantly, it is references, as any change to the value or policy network will also change the joint network and vice versa. In the model plot, we can also see how policy and value network are separated. They only share their input, but not their weights.\n",
    "\n",
    "With model, environment and distribution set up, we can now assemble an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 0 GPU devices.\n",
      "Using [StateNormalizationTransformer, RewardNormalizationTransformer] for preprocessing.\n",
      "An MPI Optimizer with 1 ranks has been created; the following ranks optimize: [0]\n"
     ]
    }
   ],
   "source": [
    "agent = apy.Agent(build_models, env, horizon=1024, workers=1, distribution=distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the agent for 10 cycle and afterwards save the final state. AngoraPy will additionally always save the agents best version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Drill started using 1 processes for 1 workers of which 1 are optimizers. Worker distribution: [1].\n",
      "IDs over Workers: [[0]]\n",
      "IDs over Optimizers: [[0]]\n",
      "Gathering cycle 0..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 240.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mBefore Training\u001b[0m: r: \u001b[91m   19.20\u001b[0m; len: \u001b[94m   19.20\u001b[0m; n: \u001b[94m 51\u001b[0m; loss: [\u001b[94m  pi  \u001b[0m|\u001b[94m  v     \u001b[0m|\u001b[94m  ent \u001b[0m]; eps: \u001b[94m    0\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m     0\u001b[0m; f: \u001b[94m   0.000\u001b[0mk; y.exp: \u001b[94m0.000\u001b[0m; times:  ; took s [unknown time left]; mem: 1.11/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 1..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 248.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     1/10\u001b[0m: r: \u001b[91m   31.48\u001b[0m; len: \u001b[94m   31.48\u001b[0m; n: \u001b[94m 31\u001b[0m; loss: [\u001b[94m  0.08\u001b[0m|\u001b[94m    0.63\u001b[0m|\u001b[94m  0.68\u001b[0m]; eps: \u001b[94m   51\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m    48\u001b[0m; f: \u001b[94m   1.024\u001b[0mk; times: [5.2|0.0|2.3] [69|0|31]; took 7.58s [1.1mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 2..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 245.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     2/10\u001b[0m: r: \u001b[91m   52.26\u001b[0m; len: \u001b[94m   52.26\u001b[0m; n: \u001b[94m 19\u001b[0m; loss: [\u001b[94m -0.06\u001b[0m|\u001b[94m    0.32\u001b[0m|\u001b[94m  0.67\u001b[0m]; eps: \u001b[94m   82\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m    96\u001b[0m; f: \u001b[94m   2.048\u001b[0mk; times: [5.1|0.0|1.0] [84|0|16]; took 6.36s [0.9mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 3..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 246.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     3/10\u001b[0m: r: \u001b[91m   85.82\u001b[0m; len: \u001b[94m   85.82\u001b[0m; n: \u001b[94m 11\u001b[0m; loss: [\u001b[94m -0.13\u001b[0m|\u001b[94m    0.22\u001b[0m|\u001b[94m  0.63\u001b[0m]; eps: \u001b[94m  101\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   144\u001b[0m; f: \u001b[94m   3.072\u001b[0mk; times: [5.1|0.0|1.0] [84|0|16]; took 6.26s [0.8mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 4..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 212.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     4/10\u001b[0m: r: \u001b[91m   99.70\u001b[0m; len: \u001b[94m   99.70\u001b[0m; n: \u001b[94m 10\u001b[0m; loss: [\u001b[94m  0.10\u001b[0m|\u001b[94m    0.14\u001b[0m|\u001b[94m  0.61\u001b[0m]; eps: \u001b[94m  112\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   192\u001b[0m; f: \u001b[94m   4.096\u001b[0mk; times: [5.1|0.0|1.0] [84|0|16]; took 6.96s [0.7mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 5..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 243.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     5/10\u001b[0m: r: \u001b[91m  115.50\u001b[0m; len: \u001b[94m  115.50\u001b[0m; n: \u001b[94m  8\u001b[0m; loss: [\u001b[94m  0.03\u001b[0m|\u001b[94m    0.07\u001b[0m|\u001b[94m  0.59\u001b[0m]; eps: \u001b[94m  122\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   240\u001b[0m; f: \u001b[94m   5.120\u001b[0mk; times: [5.8|0.0|1.0] [85|0|15]; took 6.43s [0.6mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 6..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 250.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     6/10\u001b[0m: r: \u001b[91m  162.17\u001b[0m; len: \u001b[94m  162.17\u001b[0m; n: \u001b[94m  6\u001b[0m; loss: [\u001b[94m -0.03\u001b[0m|\u001b[94m    0.06\u001b[0m|\u001b[94m  0.57\u001b[0m]; eps: \u001b[94m  130\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   288\u001b[0m; f: \u001b[94m   6.144\u001b[0mk; times: [5.2|0.0|1.2] [81|0|19]; took 6.42s [0.4mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 7..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 242.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     7/10\u001b[0m: r: \u001b[33m  276.00\u001b[0m; len: \u001b[94m  276.00\u001b[0m; n: \u001b[94m  3\u001b[0m; loss: [\u001b[94m -0.08\u001b[0m|\u001b[94m    0.03\u001b[0m|\u001b[94m  0.56\u001b[0m]; eps: \u001b[94m  136\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   336\u001b[0m; f: \u001b[94m   7.168\u001b[0mk; times: [5.0|0.0|1.0] [83|0|17]; took 6.4s [0.3mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 8..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 251.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     8/10\u001b[0m: r: \u001b[33m  399.50\u001b[0m; len: \u001b[94m  399.50\u001b[0m; n: \u001b[94m  2\u001b[0m; loss: [\u001b[94m  0.09\u001b[0m|\u001b[94m    0.02\u001b[0m|\u001b[94m  0.59\u001b[0m]; eps: \u001b[94m  139\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   384\u001b[0m; f: \u001b[94m   8.192\u001b[0mk; times: [5.2|0.0|1.0] [84|0|16]; took 6.24s [0.2mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finalizing...\r",
      "Gathering cycle 9..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering experience...: 100%|█████████████| 1024/1024 [00:04<00:00, 239.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[92mCycle     9/10\u001b[0m: r: \u001b[33m  336.33\u001b[0m; len: \u001b[94m  336.33\u001b[0m; n: \u001b[94m  3\u001b[0m; loss: [\u001b[94m -0.14\u001b[0m|\u001b[94m    0.04\u001b[0m|\u001b[94m  0.54\u001b[0m]; eps: \u001b[94m  141\u001b[0m; lr: \u001b[94m1.00e-03\u001b[0m; upd: \u001b[94m   432\u001b[0m; f: \u001b[94m   9.216\u001b[0mk; times: [5.0|0.0|1.0] [83|0|17]; took 6.45s [0.1mins left]; mem: 1.14/12|0.0/0.0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing...:   0%|                                     | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing...Drill finished after 65.84serialization.\n"
     ]
    }
   ],
   "source": [
    "agent.drill(n=10, epochs=3, batch_size=64)\n",
    "agent.save_agent_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is done, we can evaluate the agent. For this purpose we tell the agent to *act confidently*. Because policies in AngoraPy are stochastic, actions are usually sampled from the policy distribution. At evaluation time, we would however prefer the agent to stop exploring and instead choose the action it is most confident about. Thus, when told to act confidently, the agent will not sample but instead choose the most likely action under the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:10<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = agent.evaluate(10, act_confidently=True)[0]\n",
    "print(np.mean(evaluation_results.episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see (usually) performance is higher than after the last optimization, because the agent is not exploring anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
